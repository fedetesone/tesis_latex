\section{Marco teórico}
\subsection{Sitios de CQA}
\begin{frame}
	\frametitle{Sitios de CQA}
	Los servicios de \textit{Community Question Answering} CQA, son un tipo especial de servicios de \textit{Question Answering} (QA), los cuales permiten a los usuarios registrados responder a preguntas formuladas por otras personas.
\end{frame}

\begin{frame}
	\frametitle{Sitios de CQA}
	\scriptsize
	El mecanismo existente por el cual se responden las preguntas en los sitios de CQA todavía no alcanza a satisfacer las expectativas de los usuarios por varias razones:
	\bigskip
	\begin{itemize} [<+>]
		\item Baja probabilidad de encontrar al experto.
		\item Respuestas de baja calidad.
		\item Preguntas archivadas y poco consultadas: muchas preguntas de los usuarios son similares.
	\end{itemize}
\end{frame}

\subsection{Sistemas de recomendación}
\begin{frame}
	\frametitle{Sistemas de recomendación}
	Como se mencionó anteriormente, un RS es un conjunto de herramientas de software que sugiere ítems a un usuario, quien posiblemente utilizará algunos de ellos.
\end{frame}

\subsection{Sistemas de recomendación}
\begin{frame}
	\frametitle{Técnicas de Recomendación}
	Los RS basan sus estrategias de recomendaciones en 6 técnicas básicas (Ricci et al., 2011):
	\begin{itemize} [<+>]
		\item Basados en contenido.
		\item Basados en contenido.
		\item Demográficos.
		\item Basados en conocimiento.
		\item Basados en comunidades (sociales).
		\item Sistemas Híbridos.
	\end{itemize}
\end{frame}

\subsection{Big Data y Arquitecturas}
\begin{frame}
	\frametitle{Big Data}
	Conjuntos de datos cuyo tamaño está más allá de la habilidad de las herramientas software de base de datos para capturar, almacenar, gestionar y analizar los datos (Manyika et al., 2011).

	\bigskip

	``Big Data son activos de información caracterizados por su alto volumen, velocidad y variedad que demandan formas innovadoras y rentables de procesamiento de información para mejorar la compresión y la toma de decisiones'' (consultora Gartner).
\end{frame}

\begin{frame}
	\frametitle{Big Data}
	Conceptos importantes relativos a este trabajo:

	\begin{itemize} [<+>]
		\item Map-reduce.
		\item Arquitectura Hadoop (HDFS).
		\item Apache Spark.
	\end{itemize}
\end{frame}

\subsection{Medidas de distancia de texto}
\begin{frame}
	\frametitle{Information retrieval}
	\textit{Information retrieval} (IR), traducido a menudo como ``recuperación de información'', se define la acción de como encontrar material (generalmente documentos) de una naturaleza desestructurada (generalmente texto) que satisfaga una necesidad de información de grandes colecciones (generalmente almacenadas en computadoras) (Schütze et al. 2008).
\end{frame}

\begin{frame}
	\frametitle{Similaridad}
	Las medidas de similaridad son interés poder cuantificar la relación entre objetos.

	\bigskip

	La función de similaridad es definida satisfaciendo las condiciones:
	\begin{enumerate}
		\item Simetría,
		\[S(x_i,x_j)=S(x_j,x_i);\]

		\item Positividad,
		\[0 \leq S(x_i,x_j) \leq 1, \quad \forall x_i,x_j.\]
	\end{enumerate}

	\bigskip
	Es posible transformar una medida de similaridad \(S(x_i,x_j)\) en una de distancia \(D(xi,xj)\) que cumpla \(0 \leq D(x_i,x_j) \leq 1\), en el intervalo \([0,1]\). Aplicando \(D(x_i,x_j) = 1 - S(x_i,x_j)\).
\end{frame}

\begin{frame}
	\frametitle{Modelo de espacio vectorial}
	En el modelo de \textit{espacio vectorial}, un texto es representado como un vector de términos. Si las palabras son elegidas como términos, entonces cada palabra del vocabulario sería una \textit{dimensión} independiente en el espacio vectorial (Singhal et al., 2001).

	\bigskip

	Típicamente, el ángulo entre los dos vectores es usado como medida de divergencia entre los mismos, y el coseno del ángulo es usado como similaridad numérica.
\end{frame}

\begin{frame}
	\frametitle{Distancia del coseno}
	Siendo \(\overrightarrow{D_i}\) y \(\overrightarrow{D_j}\) dos documentos en forma de vectores:
	\[d_c(\vec{D}_i, \vec{D}_j) = 1 - cos(\vec{D}_i, \vec{D}_j) = 1 - \frac{{\vec{D}_i}'\vec{D}_j}{\sqrt{{\vec{D}_i}'\vec{D}_i}\sqrt{{\vec{D}_j}'\vec{D}_j}}.\]

	\bigskip
	Particularmente en IR, es de interés el intervalo \([0, 1]\), entonces la distancia del coseno se puede derivar de la fórmula del \textit{producto escalar}:
	\[\vec{D}_i.\vec{D}_j = \left \| \vec{D}_i \right \|.\left \| \vec{D}_j \right \|.cos(\theta),\]
	siendo \(\left \|\overrightarrow{D_i}\right \|\) y \(\left \|\overrightarrow{D_j}\right \|\) los módulos de los vectores \(\overrightarrow{D_i}\) y \(\overrightarrow{D_j}\) respectivamente, y $\theta$ el ángulo formado entre ellos
\end{frame}

\begin{frame}
	\frametitle{Term Frequency (TF)}
	\begin{itemize}
		\scriptsize
		\item También conocido en la literatura como \textit{Bag of words} (bolsa de palabras).
		\item El orden exacto de los términos es ignorado, pero se basa en el número de ocurrencias de cada uno de ellos en un documento.
		\item Cada documento corresponde a un vector y cada término a una dimensión.
		\item Se mide el grado de similaridad de dos documentos utilizando el coseno del ángulo.
	\end{itemize}

	\bigskip
	\centering
	\textit{“Mary is quicker than John”} y \textit{“John is quicker than Mary”}
\end{frame}

\begin{frame}
	\frametitle{Term Frequency (TF)}
	\begin{itemize}
		\scriptsize
		\item Se define \textit{document frequency} \(df_t\) como el número de documentos en una colección que contienen el término \(t\).
		\item \textit{Inverse document frequency}, o IDF, es un indicador basado en la cantidad de documentos que contienen (o son indexados por) un término en cuestión.
		\item Intuición: si un término de búsqueda se encuentra en muchos documentos, no es un buen discriminador, y se le debe asignar menor peso que a un término que se encuentra en pocos documentos.
	\end{itemize}

	\bigskip
	\centering
	\[tfidf(t_i, d_j) = tf(t_i, d_j) \cdot idf(t_j)\]
\end{frame}

\begin{frame}
	\frametitle{Word2Vec}
	\begin{itemize}
		\scriptsize
		\item Modelos basados en redes neuronales con una capa oculta para computar representaciones de palabras como vectores continuos en grandes conjuntos de datos.
		\item Dos modelos: Skip-gram y Continuous Bag of Words.
		\item Las entradas y salidas de la red neuronal son palabras representadas como \textit{one-hot} vector.
		\item Los pesos de la capa oculta se van ajustando utilizando un clasificador de regresión Softmax.
		\item Estos pesos resultantes dan como resultado a la representación vectorial de palabras utilizadas para el cálculo de similaridad de este trabajo.
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{FastText}
	\begin{itemize}
		\scriptsize
		\item
	\end{itemize}
\end{frame}