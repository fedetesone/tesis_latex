\newpage \section{Fundamentación}\label{ch:fundamentacion}
\subsection{Estado del arte}
\subsubsection{Sitios de CQA}
\noindent Los servicios de Community Question Answering CQA, son un tipo especial de servicios \textit{Question Answering} (QA), los cuales permiten a los usuarios registrados responder a preguntas formuladas por otras personas. Los mismos atrajeron a un número creciente de usuarios en los últimos años \citep{li2010routing}. Una pregunta formulada en Quora, y respondida por su fundador y CEO, Adam D\textsc{\char13}Angelo, revela que el sitio recibe más de 200 millones de visitantes únicos mensualmente (información actualizada a Junio de 2017), lo que denota la popularidad de este tipo de portales\footnote{Pregunta formulada en el sitio Quora \textit{``How many people use Quora?''}: \url{https://www.quora.com/How-many-people-use-Quora-7}. Último acceso Agosto 2018.}. Desde la creación de este tipo de servicios, se han aplicado diferentes técnicas de software para que los usuarios encuentren respuestas a sus preguntas en el menor tiempo posible y aprovechar al máximo el valor de las bases de conocimiento, por ejemplo, un framework para predecir la calidad de las respuestas con características no textuales \citep{jeon2006framework}, incorporar información de legibilidad en el proceso de recomendación \citep{anuyah2017can}, encontrar a los expertos apropiados \citep{li2010routing} o recomendar la mejor respuesta a una pregunta dada, entre otros. Sin embargo, el mecanismo existente en el cual se responden las preguntas en los sitios de CQA todavía no alcanza a satisfacer las expectativas de los usuarios por varias razones: 
\begin{inparaenum}[(1)]
	\item baja probabilidad de encontrar al experto: una nueva pregunta, en muchos casos, puede no encontrar a la persona con la habilidad de responderla de manera correcta, resultando en respuestas tardías y que distan de ser óptimas; 
	\item respuestas de baja calidad: los sitios de CQA suelen contener respuestas de baja calidad, maliciosas y spam. Estas suelen recibir baja calificación de los miembros de la comunidad; \item preguntas archivadas y poco consultadas: muchas preguntas de los usuarios son similares. Antes de formular una pregunta, un usuario podría beneficiarse de buscar ya formuladas, y por consiguiente, sus respuestas \citep{yang2013cqarank}.
\end{inparaenum}

\subsubsection{Sistemas de recomendación}
\noindent Es muy frecuente tener que tomar decisiones sin la suficiente experiencia personal sobre las alternativas disponibles. En la vida cotidiana, confiamos en recomendaciones de otras personas ya sea de boca en boca o cartas de recomendación, reseñas de libros y películas o encuestas generales. Los sistemas de recomendación asisten este proceso natural en el ámbito de los sistemas de información \citep{resnick1997recommender}. El primer RS, \textit{Tapestry} \citep{goldberg1992using}, fue un sistema experimental de correo electrónico destinado a resolver el problema de manejar grandes cantidades de emails filtrando según cuán interesantes son los documentos, utilizando un enfoque basado en el contenido de los mismos y también filtros colaborativos, lo que después se denominaría RS no personalizados y personalizados por \citeauthor{ricci2011introduction} en el año \citeyear{ricci2011introduction}. Se ha trabajado mucho en mejorar y desarrollar nuevos enfoques con respecto a RS en los últimos años, y el interés en esta área sigue vigente debido a la abundancia de aplicaciones prácticas en las cuales es necesario ayudar a los usuarios a lidiar con la sobrecarga de información\footnote{El concepto de sobrecarga de información, del inglés \textit{information overload}, hace referencia a cuando los usuarios reciben demasiada información, por lo cual, la precisión en sus decisiones empieza a decrecer \citep{eppler2004concept}.} y proveer recomendaciones personalizadas, contenidos y servicios. Sin embargo, a pesar de todos estos avances, la generación actual de RS todavía requiere mejoras para que los métodos de recomendación sean más efectivos y aplicables a una gama más amplia de sistemas y/o sitios. Aunque las raíces de los RS se remontan a trabajos en ciencia cognitiva \citep{rich1979user}, teoría de aproximación \citep{powell1981approximation}, recuperación de información \citep{salton1989automatic}, ciencias de las predicciones \citep{armstrong2001principles}, ciencias de la gestión \citep{murthi2003role} y también al modelado de la elección de consumidor en marketing \citep{lilien1992marketing}, los RS recién surgen como un área de investigación independiente en la década de 1990, cuando los investigadores comenzaron a centrarse en problemas de recomendación que se basan específicamente en calificaciones \citep{adomavicius2005toward}. En su formulación más común, el problema de recomendación se reduce a estimar calificaciones para los ítems que no han sido vistos por un usuario.

\subsubsection{Big Data}
\noindent Al igual que todos los términos que surgen a partir de avances tecnológicos, no existe un consenso claro de cómo definir \textit{Big Data}. \cite{manyika2011big} definen este concepto como los conjuntos de datos cuyo tamaño está más allá de la habilidad de las herramientas software de base de datos para capturar, almacenar, gestionar y analizar. Nótese que esta definición es agnóstica del tamaño del conjunto de datos, y no define un tamaño mínimo del mismo, sino que, asume que la tecnología avanza constantemente como así también las herramientas, por lo cual, la definición se “mueve” con el tiempo. Por otro lado, también es interesante tomar otra arista en la definición de este concepto. La consultora Gartner en su sitio web\footnote{Concepto de Big Data en el glosario de Gartner: \url{https://www.gartner.com/it-glossary/big-data}. Último acceso Agosto 2018.} lo define como “Big Data son activos de información caracterizados por su alto volumen, velocidad y variedad que demandan formas innovadoras y rentables de procesamiento de información para mejorar la compresión y la toma de decisiones”, haciendo énfasis en la multiplicidad de características de Big Data. 

\bigskip El comienzo de sobrecarga de información, recientemente mencionado, data del año 1880, cuando el censo de los Estados Unidos tarda 8 años en tabularse. Ante esta situación Herman Hollerith inventó la máquina tabuladora eléctrica basada en tarjetas perforadas\footnote{Herman Hollerith, US Census Boureau: \url{https://www.census.gov/history/www/census_then_now/notable_alumni/herman_hollerith.html}. Último acceso Agosto 2018.}. El censo en 1890 fue un éxito rotundo e, incluso, la máquina que él diseñó fue  utilizada para los censos de Canadá, Noruega y Austria al año siguiente.

\bigskip En el año 1941, los científicos empiezan a utilizar el término “explosión de la información”, que fuera  citado en el periódico The Lawton Constitution\footnote{The Lawton Constitution: \url{http://www.swoknews.com/}. Último acceso Agosto 2018.}, haciendo alusión a la dificultad de administrar toda la información disponible. Gradualmente, se identificaron avances concretos en materia de procesamiento de datos y criptografía, motivados particularmente por los sucesos bélicos de la época. Un ejemplo es el dispositivo llamado Colossus \citep{copeland2004colossus} que buscaba e interceptaba mensajes a una tasa de miles de caracteres por segundo. Unos años más tarde, en 1951 el concepto de \textit{memoria virtual} es introducido por el físico alemán Fritz-Rudolf Güntsch, como una idea que trataba el almacenamiento finito como infinito.

\bigskip A partir de la década del 80', los avances tecnológicos, especialmente en sistemas MRP (planificación de recursos de fabricación), permitieron nuevas formas de organizar, almacenar y generar datos. En este sentido, IBM se destaca y define una arquitectura para los informes y análisis de negocio (EBIS)\footnote{Acrónimo para EMEA (Europe, Middle East and Africa) Business Information System.}, que se convierte en la base del almacenamiento de datos en forma centralizada para usuarios finales \citep{devlin1988architecture}; es decir, el \textit{data warehousing}. Hacia finales de los 80’, Tim Berners-Lee, inventa la \textit{World Wide Web} \citep{berners1992world}. Invento que implicaría el impacto más grande hasta la actualidad con respecto a la generación, identificación, almacenamiento y análisis de grandes volúmenes de datos de diversa naturaleza.

\bigskip El inicio de los años 90’ marcan un antes y un después en lo relativo al tratamiento y almacenamiento de datos. El crecimiento tecnológico fue explosivo, tal es así que el almacenamiento digital empieza a ser más conveniente y rentable que el papel para almacenar datos \citep{morris2003evolution}. Es en 1990 cuando surgen las plataformas de \textit{Business Intelligence} (BI) y los rediseños de software al estilo \textit{Enterprise Resource Planning} (ERP). En este contexto, \cite{cox1997application} afirman que el crecimiento de la cantidad de datos que debe manejar un sistema de información empieza a ser un problema en materia de almacenamiento y visualización de los datos, situación que denominaron como “el problema del Big Data”. Así, 1997 es un año clave, en el que se realizan un gran porcentaje de estudios y publicaciones que se enfocan en averiguar cuánta información hay disponible a nivel mundial y su crecimiento\footnote{Michael Lesk publica “How much information is there in the world?” (1997): \url{http://www.lesk.com/mlesk/ksg97/ksg.html}. Último acceso Agosto 2018.}, y, en consecuencia, se estima que el crecimiento de Internet es aproximadamente del 100\% anual y que superaría el tráfico de voz para el año 2002 \citep{coffman1998size}.

\bigskip En el año 2001, se introduce el concepto de \textit{las 3 V’s: Volumen, Velocidad y Variabilidad de los datos} \citep{laney20013d} fundantes sobre la temática y que sería mundialmente aceptado una década más tarde. Por otro lado, también, en 2001 aparece el concepto de \textit{Software como un Servicio} (SaaS) \citep{hoch2001software}, un modelo disruptivo de servicios centralizados y acceso a los mismos mediante clientes finos (típicamente exploradores web), dando la posibilidad del escalamiento horizontal de sistemas de información y la generación de estándares de comunicación. Esta situación provocó que empresas como Oracle\footnote{Oracle: \url{https://www.oracle.com}. Último acceso Agosto 2018.}, SAP\footnote{SAP: \url{https://www.sap.com}. Último acceso Agosto 2018.}, y Peoplesoft\footnote{Peoplesoft: adquirida por Oracle en Enero de 2005.} empiecen a centrarse en el uso de servicios web, permitiendo así la generación de datos en forma masiva por usuarios finales. Así, en 2006, nace Apache Hadoop\footnote{Apache Hadoop: \url{http://hadoop.apache.org/}. Último acceso Agosto 2018.}, una solución de código abierto que permite el procesamiento en paralelo y distribuido de enormes cantidades de datos en forma escalable. Posteriormente, en 2008, se empieza a pensar al Big Data como la mayor innovación en informática en la última década, ya que ha transformado la forma en que los motores de búsqueda acceden a la información, las actividades de las compañías, las investigaciones científicas, la medicina, y las operaciones de defensa e inteligencia de los países, entre otras tantas actividades. Más aún, se ha comenzado a ver su potencial para recopilar y organizar datos en todos los ámbitos de la vida cotidiana \citep{bryant2008big}, tales como redes sociales, estadísticas deportivas o avances médicos y genéticos.

\subsection{La propuesta}
\noindent La calidad de un RS tiene una relación directa con los datos de entrada que se han generado para alimentarlo. Con el fin de generar una entrada basada en medidas de similaridad, es necesaria la comparación de preguntas formuladas en sitios de CQA usando técnicas de análisis de texto.
Un problema importante inherente al análisis de texto, con el fin de cuantificar relaciones entre distintos fragmentos o documentos, es encontrar la medida apropiada de representación. Algunas medidas de similaridad resultantes de algoritmos de recomendación en análisis de texto, son obtenidas mediante algoritmos puramente sintácticos, léxicos, tales como: \textit{Term Frequency} \citep{salton5mcgill}, \textit{Term Frequency/Inverse Document Frequency} \citep{baeza1999modern}, basados en ventanas como \textit{FastText} \citep{joulin2016fasttext} o \textit{Word2Vec} \citep{mikolov2013efficient}, o semánticos, como \textit{Semantic Distance} \citep{li2006sentence}. Los algoritmos puramente sintácticos como Term Frequency y Term Frequency/Inverse Document Frequency tienen conocidos problemas, tales como ser invariantes respecto al orden de las palabras o ser sensibles a stopwords, por lo cual, necesitan un gran trabajo de pre-procesamiento. FastText y Word2Vec están fuertemente afectados en el orden en el cual aparecen las palabras. Adicionalmente, ninguna de estas técnicas tiene en cuenta la semántica de las palabras y sus relaciones, como si lo hace Semantic Distance. Sin embargo, esta última técnica, según el trabajo tomado como estado del arte, tampoco alcanza medidas de rendimiento apropiadas para un RS en un sitio de CQA.

\bigskip Resultados experimentales de medidas de rendimiento obtenidas en el trabajo que se toma como punto de partida de esta tesis, arrojan entre un 66\% y un 68\% de precisión y entre un 32\% y un 33.5\% de error usando cada uno de los algoritmos de recomendación descritos anteriormente. Estos valores son considerados prometedores, ya que las medidas de rendimiento son consistentes en todos los algoritmos seleccionados, lo que denota que la complejidad inherente del conjunto de datos no afecta significativamente la performance de cada uno de ellos. Además, los resultados de prueba no varían significativamente con respecto a los resultados de validación. Dicho esto, la motivación de este trabajo de tesis, así como el de las futuras líneas de investigación, es la creación de una medida de similaridad de texto novedosa que sirva como entrada para un RS aplicable a sitios de CQA. Para tal fin, se crearán matrices de distancias, usando cada una de las preguntas del conjunto de datos en estudio, para luego combinarlas usando métodos de ensamble de clustering, ya que, como existen cientos algoritmos de clustering, es difícil identificar un solo algoritmo que pueda manejar todos los tipos de forma y tamaños de cluster, e incluso, decidir qué algoritmo sería el mejor para un conjunto de datos en particular. \cite{fred2005combining} introducen el concepto de clustering de acumulación de evidencias, que mapea las particiones de datos individuales en un ensamble de clustering dentro de una nueva medida de similaridad entre patrones, sumarizando la estructura entre-patrón percibido de esos clusters. La partición de datos final es obtenida aplicando el método \textit{single-linkage} a la nueva matriz de similaridad. El resultado de este método muestra que, la combinación de algoritmos de clustering “débiles” como el \textit{k-means}, pueden conducir a la identificación de clusters subyacentes verdaderos con formas, tamaños y densidades arbitrarias. Por lo cual, teniendo en cuenta diferentes particiones creadas con el método de ensamble desde los mismos datos originales, objetos de textos similares probablemente pertenecerán al mismo cluster.

\bigskip El desarrollo de matrices de similaridad para la aplicación del EAC que se utilizarán como entrada de RS, claramente implica manipular un gran volumen de datos complejos y realizar un elevado número de cálculos en tiempo real, ya que nos estamos refiriendo a conjuntos de datos cuyo tamaño supera la capacidad de las herramientas tradicionales de bases de datos de recopilar, almacenar, gestionar y analizar la información \citep{de2016mineria}. Esto implica, en principio, considerar una \textit{matriz de co-asociación} entre elementos realizando varias series de corridas y aplicación de clustering. Cada una de esas series es basada en una de las medidas de similaridad. El resultado será un un valor adimensional e insesgado que puede mejorar la representación para la estructura subyacente de relaciones de texto. El volumen de datos ejemplificado en las secciones anteriores, deja expuesta necesidad de investigar y desarrollar el tema aquí propuesto con un enfoque distinto al tradicional. Esto implica realizar un muestreo aleatorio de pares de preguntas dentro de una arquitectura que permita generar la mayor cantidad posible de subconjuntos de datos extraídos aleatoriamente. Además, posibilitará que cada uno de ellos sea lo más grande posible para aprovechar toda la variedad de los datos. Mientras más se aproveche la variedad de los datos (más subconjuntos de datos y de mayor tamaño), más afectará negativamente en el tiempo de de procesamiento, razones por las cuales se hace necesaria una arquitectura e infraestructura preparada para tal desafío, con una \textit{velocidad} que haga posible obtener resultados en un período de tiempo razonablemente corto. Un enfoque Big Data es imprescindible  para este tipo de procesamiento de datos. No solo se desea hacer referencia a la gran cantidad y complejidad de los datos, sino también a las herramientas utilizadas para procesarlos y las posibilidades de extraer conocimiento útil a partir del análisis de los mismos. Estos procesos y herramientas son el eje central de la definición de Big Data de la consultora Gartner (2012), la cual hace foco en los procesos para manipular activos de gran volumen y variedad con una gran velocidad. Por lo cual, si bien Big Data se refiere a estos activos, demanda formas innovadoras y efectivas de procesarlos, que habiliten tomas de decisiones y automatización de procesos.

\bigskip Por todos estos motivos, se propone la elaboración de un nuevo método y una arquitectura que lo soporte, que genere una entrada de datos correctamente estructurada para RS y que pueda ser utilizada da en sitios de CQA, de una forma eficiente y eficaz.
