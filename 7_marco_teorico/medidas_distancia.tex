\subsection{Medidas de distancia de texto}
\subsubsection{Conceptos básicos}
\paragraph{Information retrieval}
Information retrieval (IR) se define como encontrar material (generalmente documentos) de una naturaleza desestructurada (generalmente texto) que satisfaga una necesidad de información de grandes colecciones (generalmente almacenadas en computadoras) \citep{schutze2008introduction}.

\bigskip El IR utiliza técnicas probabilísticas, pero también, en los últimos años, investigadores se han centrado en técnicas basadas en conocimiento. Estas últimas, han hecho una significante contribución al IR “inteligente”. Más recientemente, la investigación se ha volcado a nuevas técnicas de aprendizaje inductivo basadas en \textit{inteligencia artificial} (IA), las cuales incluyen redes neuronales, aprendizaje simbólico y algoritmos genéticos \citep{chen1995machine}. Cuando hablamos de aprendizaje, nos referimos a un fenómeno multifacético. Los procesos de aprendizaje incluyen la adquisición de un nuevo conocimiento declarativo, la organización del nuevo conocimiento, representaciones efectivas y un descubrimiento de nuevos hechos y teorías a través de la observación y la experimentación. Desde el nacimiento de la era de las computadoras, estas capacidades han querido ser implantadas en las mismas. El estudio y el modelado en computadoras del proceso de aprendizaje en múltiples manifestaciones constituye el propósito principal del \textit{Machine Learning} (ML) \citep{mitchell2013artificial}.

\bigskip Los Sistemas de Recomendación a los cuales se hace foco en este trabajo, aplican técnicas que no son más que conceptos derivados del IR y algunos del ML. Técnicas probabilísticas y determinísticas son utilizadas para extraer información relevante desde diferentes orígenes de datos, así como también generar información valiosa a partir de ellos. Para este último propósito, se han desarrollado métodos basados en el aprendizaje inductivo que demostraron ser muy efectivos y, en algunos casos, tener modelos simples y aplicables que permiten desarrollar RS eficaces y escalables.

\paragraph{Unidad de documento}
Una \textit{unidad de documento}, es una secuencia de caracteres de longitud fija con las cuales se va a trabajar. Estas secuencias de caracteres pueden estar codificadas por uno o varios bytes o esquemas de codificación multibyte, como UTF-8 o varios estándares específicos de algún país o compañía. Una vez que la codificación esté determinada, se debe decodificar la secuencia de bytes a una secuencia de caracteres. 

\paragraph{Stopwords}
En informática, se llama stopword a palabras que se filtran antes o después del procesamiento de datos del lenguaje natural \citep{leskovec2014mining}. Generalmente, este tipo de palabras son extremadamente comunes, y podrían tener poco valor en el momento de seleccionar documentos que coincidan con las necesidades de un usuario \citep{schutze2008introduction}.

\bigskip Entonces es necesario seleccionar una lista de stopwords, que serán filtradas en el procesamiento de las unidades de documento, estas listas son llamadas \textit{stop lists}. La estrategia general para el armado de stop lists, es ordenar los términos por \textit{collection frequency}, es decir, el número total de veces que aparece un término en la colección de documentos. Una vez hecho esto, es necesario tomar los términos más frecuentes, a veces filtrados a manos según su contenido semántico relativo al dominio de los documentos que están siendo indexados.

\paragraph{Tokenización}
Dado una secuencia de caracteres y una unidad de documento definida, la \textit{tokenización} es la tarea de dividirla en distintas piezas, llamadas \textit{tokens}, y, quizás en el mismo momento, desechar ciertos caracteres (como signos de puntuación). Un ejemplo de tokenización de una secuencia de caracteres en idioma inglés, podría ser:

\begin{verbatim}
Input: Friends, Romans, Countrymen, lend me your ears;
Output: [Friends] [Romans] [Countrymen] [lend] [me] [your] [ears]
\end{verbatim}

Estos tokens, son frecuentemente confundidos con palabras, pero es importante hacer la distinción entre token y tipo. Un token es una instancia de una secuencia de caracteres en un documento en particular que están agrupados juntos como una unidad semántica útil para procesamiento. Un \textit{tipo} es la clase de todos los tokens que contienen la misma secuencia de caracteres. Un \textit{término} es un tipo, que es incluido en un diccionario de un sistema de IR. Por ejemplo, si un documento a ser indexado es “to sleep perchance to dream”, existen 5 tokens, pero solo 4 tipos (ya que hay dos instancias del token “to”). Sin embargo, si “to” es desechada por ser un stopword, habrá entonces 3 términos: “sleep”, “perchance” y “dream”.

\paragraph{Similaridad}
Es de interés poder cuantificar la relación entre los objetos de texto. Existen distintas maneras de medir esa relación. En general se habla de medidas de proximidad \citep{xu2008clustering}. Las medidas de proximidad son una generalización para las medidas de similaridad y disimilaridad. En este trabajo utilizaremos medidas de similaridad comúnmente encontradas en la literatura \citep{resnik1995using, lin1998information, gomaa2013survey, harispe2015semantic}. Debido al propósito de este trabajo, es de interés, en particular, la similaridad semántica en taxonomías \citep{resnik1995using}, que se basa en el contenido de información de las unidades documentales.

\bigskip Las medidas basadas en contenido de información de una unidad de documento (o concepto) en una taxonomía, utilizan el siguiente enfoque. Se define \(p(t)\) como:
\[p(t)=\frac{cantidad\>de\>palabras\>asociadas\>a\>una\>definici\acute{o}n\>\>sus\>hijos}{cantidad\>total\>de\>palabras\>en\>el\>corpus}\]

\bigskip De esta forma, el nodo raíz tendrá \(p(t) =0\), mientras los nodos hojas, tendrán valores cercanos a 1. Se define el contenido de información \(I(t) =0\) como:
\[I(t)=-\log p(t)\]

\bigskip Aplicando el logaritmo negativo a \(p(t) =0\) se logra que los nodos hoja, siendo conceptos muy específicos, contengan mucha información, y que los nodos más genéricos que se encuentran cerca de la raíz contengan un contenido de información que, por lo contrario, tienda a cero.

\bigskip \cite{resnik1995using} define, en un conjunto de conceptos \(C\) en una taxonomía \textit{es-un}, que permite herencia múltiple, que la similaridad entre dos conceptos es la medida en que ellos comparten información en común, indicado, en este tipo de taxonomías, por el nodo inmediato de más alto nivel que los subsume a ambos, el \textit{subsumidor mínimo} (minimum subsumer o ms por sus siglas en inglés). 

\bigskip Considerando dos términos \(t_i\) y \(t_j\), y a \(S(t_i, t_j\)) al conjunto de ancestros comunes de \(t_i\) y \(t_j\), se define al subsumidor mínimo, \(ms(t_i, t_j)\), como al término de \(S(t_i, t_j)\) que contiene el máximo contenido de información:

\[\max_{t \in s(t_i,t_j)} I(t) = I(ms(t_i,t_j))\]

\bigskip La medida de similaridad de \cite{resnik1995using} \(S_R\), es entonces, el contenido de información del subsumidor mínimo de dos términos:

\[S_R = I(ms(t_i,t_j))\]

\bigskip El enfoque anterior, posee la siguiente particularidad: no tiene en cuenta la similaridad de los nodos con respecto a su subsumidor mínimo. Es intuitivo pensar que dos conceptos abstractos (nodos ubicados en posiciones más cercanas al subsumidor mínimo) son más parecidos entre sí que dos conceptos específicos (más alejados del subsumidor mínimo) \citep{lin1998information}. Para solucionar esto, \cite{lin1998information} tiene en cuenta dos aspectos para calcular la similaridad entre dos conceptos en este tipo de taxonomías: \begin{enumerate*} [label=(\roman*)] \item La cantidad de información; y \item La ubicación relativa entre los nodos hijos respecto al subsumidor mínimo. Definida como la similaridad de \cite{resnik1995using}. \end{enumerate*}

\bigskip La medida de Lin es: 

\[S_L(t_i, t_j)=\frac{2S_R(t_i,t_j)}{I(t_i)+I(t_j)}\]

\bigskip El resultado de esta medida, estará normalizada en el rango \([0, 1]\) y obtiene que nodos más generales, con menor cantidad de información, son más similares entre sí, que dos nodos específicos (ya que la cantidad de información de los mismos aumentará el denominador de la ecuación) para el mismo subsumidor mínimo. Se ha demostrado que esta definición de similaridad produce una correlación ligeramente mayor con los juicios humanos.

\paragraph{Medidas de proximidad}
Proximidad es la generalización de similaridad y disimilaridad. La función disimilaridad, también conocida como función de distancia, en un conjunto de datos X, es definida para satisfacer las condiciones. Las condiciones mencionadas, son las utilizadas por \cite{xu2008clustering} y de relevancia en el presente trabajo:

\begin{enumerate}
	\item Simetría,
	\[D(x_i,x_j)=D(x_j,x_i);\]
	
	\item Positividad,
	\[D(x_i,x_j) \geq 0 \quad \forall x_i,x_j;\]
\end{enumerate}

De forma, análoga la función de similaridad es definida satisfaciendo las condiciones:
\begin{enumerate}
	\item Simetría,
	\[S(x_i,x_j)=S(x_j,x_i);\]
	
	\item Positividad,
	\[0 \leq S(x_i,x_j) \leq 1, \quad \forall x_i,x_j\]
\end{enumerate}

Si bien el término matemático de distancia exige una serie de supuestos rigurosos \citep{xu2008clustering}, en este trabajo utilizaremos la noción de distancia y de disimilaridad en forma indistinta, y nos basaremos en las medidas de proximidad habitualmente utilizadas para comparación de texto. Por lo tanto, Para transformar una medida de similaridad \(S(x_i,x_j)\) en una de distancia \(D(xi,xj)\) que cumpla \(0 \leq D(x_i,x_j) \leq 1\), haremos la normalización de la misma en el intervalo \([0,1]\) y luego aplicaremos el cálculo \(D(x_i,x_j) = 1 - S(x_i,x_j)\) y recíprocamente \citep{leale2013novel}.

\paragraph{Modelo de espacio vectorial}
En el modelo de espacio vectorial, un texto es representado como un vector de términos. Si las palabras son elegidas como términos, entonces cada palabra del vocabulario sería una dimensión independiente en el espacio vectorial \citep{singhal2001modern}. Todo texto puede ser representado por un vector en este espacio dimensional. Si un término pertenece a un documento, éste obtiene un valor distinto de cero en el vector, junto con la dimensión correspondiente al término. Como un documento contiene un conjunto limitado de términos (el vocabulario puede contener millones de términos), muchos de los vectores pueden ser muy dispersos. La mayoría de los sistemas basados en vectores trabajan en el cuadrante positivo, es decir, a ningún término se le asigna un valor positivo. 

\bigskip Para asignar un valor numérico a un documento en una consulta, el modelo mide la similaridad entre el vector ingresado en ella y el vector del documento al cual se quiere consultar. La similaridad entre dos vectores no es inherente al modelo. Típicamente, el ángulo entre los dos vectores es usado como medida de divergencia entre los mismos, y el coseno del ángulo es usado como similaridad numérica, ya que el coseno tiene la propiedad de ser tener resultado 1 cuando los vectores son idénticos y 0 cuando los vectores son ortogonales (explicado en detalle más adelante). Como una alternativa, el producto escalar entre dos vectores, es también usado como medida de similaridad. Si todos los vectores están forzados a tener longitud 1, es decir, vectores unitarios, entonces el coseno del ángulo entre los vectores, tiene el mismo resultado que el producto escalar.

\bigskip Si \(\overrightarrow{D}\) es el vector del documento y \(\overrightarrow{Q}\) es el vector de la consulta, la similaridad entre \(\overrightarrow{D}\) y \(\overrightarrow{Q}\) es representada como:
\[S(\vec{D},\vec{Q})=\sum_{ti \in Q,D}^{ }{W_{t_{iQ}}.W_{t_{iD}}}\]

donde \(W_{t_{i \overrightarrow{Q}}}\) es el valor de la componente número \(i\) del vector \(\overrightarrow{Q}\) y \(W_{t_{i \overrightarrow{D}}}\) es el valor de la componente número \(i\) del vector \(\overrightarrow{D}\). También definido como el peso del término \(i\) en el documento \(D\). Cualquier término no presente en la consulta o en el documento tendrá valor cero en \(W_{t_{i \overrightarrow{Q}}}\) o \(W_{t_{i \overrightarrow{D}}}\), por lo cual es posible hacer la sumatoria solo de los términos en común entre la consulta y el documento.

\paragraph{Distancia del coseno}
La distancia del coseno puede ser la mayor frecuentemente aplicada en términos de similaridad en IR \citep{korenius2007principal}. Al aplicar la distancia del coseno se obtiene un resultado que se encuentra en el rango \([-1, 1]\). El valor \(-1\) significa que los vectores tienen la misma dirección, pero sentidos opuestos. El valor \(1\), por lo contrario, significa que el ángulo comprendido entre los vectores es cero.

\bigskip Particularmente en IR, es de interés el intervalo \([0, 1]\) ya que todos los componentes de un vector que representa a un documento, son no negativos. De esta interpretación, se deriva la definición de la distancia del coseno restando la medida del coseno, de su máximo valor:
\[d_c(\vec{D}_i, \vec{D}_j) = 1 - cos(\vec{D}_i, \vec{D}_j) = 1 - \frac{{\vec{D}_i}'\vec{D}_j}{\sqrt{{\vec{D}_i}'\vec{D}_i}\sqrt{{\vec{D}_j}'\vec{D}_j}}\]

donde \(i \leq i,j \leq n\). Los símbolos \(\overrightarrow{D_i}\) y \(\overrightarrow{D_j}\) son documentos en forma de vectores y dces la distancia entre ellos.

\bigskip Para simplificar, y teniendo en cuenta que estamos hablando de documentos en forma de vectores, la distancia del coseno se puede derivar de la fórmula del \textit{producto escalar} (producto punto). Siendo \(\overrightarrow{D_i}\) y \(\overrightarrow{D_j}\) dos documentos en forma de vectores, se define el producto escalar entre ellos, como:
\[\vec{D}_i.\vec{D}_j = \left \| \vec{D}_i \right \|.\left \| \vec{D}_j \right \|.cos(\theta)\]

siendo \(\left \|\overrightarrow{D_i}\right \|\) y \(\left \|\overrightarrow{D_j}\right \|\) los módulos de los vectores \(\overrightarrow{D_i}\) y \(\overrightarrow{D_j}\) respectivamente, y $\theta$ el ángulo formado entre ellos. Entonces:
\[cos(\theta) = \frac{\vec{D}_i.\vec{D}_j}{\left \| \vec{D}_i \right \|.\left \| \vec{D}_j \right \|}=\frac{\sum_{i=1}^{n}{{d_i}_i{d_j}_i}}{\sqrt{\sum_{i=1}^{n}{{d_i}_i^{2}}}\sqrt{\sum_{i=1}^{n}{{d_j}_i^{2}}}}\]

Donde \(d_i\) y \(d_j\) son los componentes de los vectores \(\overrightarrow{D_i}\) y \(\overrightarrow{D_j}\) respectivamente.

